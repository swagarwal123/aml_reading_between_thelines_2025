{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk import WordNetLemmatizer\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZdusOp7HFEBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31652c4f-57aa-4498-da93-0782ec55092b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ],
      "metadata": {
        "id": "eCxuru2wWks7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6C8yoBdfwlf",
        "outputId": "b9da5c93-55c5-408e-e672-ca075605de39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# file locations\n",
        "TRAIN_PATH = 'train.csv'\n",
        "TEST_PATH = 'test.csv'\n",
        "SAMPLE_SUB_PATH = 'sample_submission.csv'\n",
        "OUTPUT_SUB_PATH = \"submission.csv\"\n",
        "\n",
        "# model hyperparameters\n",
        "EMB_DIM = 300          # upgraded from 100\n",
        "HIDDEN_SIZE = 256      # upgraded from 128\n",
        "MAX_SEQ_LEN = 384      # longer sequences\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 15            # we'll early-stop using validation\n",
        "LR = 5e-4              # slightly smaller LR\n",
        "DROPOUT = 0.3\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# set seeds\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# use GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and split data"
      ],
      "metadata": {
        "id": "2GegVewXW1pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_df = pd.read_csv(TRAIN_PATH)\n",
        "test_df = pd.read_csv(TEST_PATH)\n",
        "sample_sub_df = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "  full_df, test_size=0.1, random_state=RANDOM_SEED, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Val shape:\", val_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahZBWSYWgFHX",
        "outputId": "3f98e4fc-bdb4-490e-ab50-fad30db5ec54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (4174, 5)\n",
            "Val shape: (464, 5)\n",
            "Test shape: (500, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "v_YyO1QQW_p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str):\n",
        "  # tokenizes a single string\n",
        "  doc = nlp(str(text))\n",
        "  tokens = []\n",
        "\n",
        "  for tok in doc:\n",
        "    # skip space\n",
        "    if tok.is_space:\n",
        "        continue\n",
        "    tokens.append(tok.text.lower())\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def lemmatize(words):\n",
        "  # Reduces words to their root form\n",
        "  wnl = WordNetLemmatizer()\n",
        "  return [wnl.lemmatize(word) for word in words]\n",
        "\n",
        "# create vocab\n",
        "counter = collections.Counter()\n",
        "\n",
        "def update_counter(df: pd.DataFrame):\n",
        "  for _, row in df.iterrows():\n",
        "    counter.update(lemmatize(tokenize(row[\"context\"])))\n",
        "    counter.update(lemmatize(tokenize(row[\"question\"])))\n",
        "    answers = ast.literal_eval(row[\"answers\"])\n",
        "    for opt in answers:\n",
        "      counter.update(lemmatize(tokenize(opt)))\n",
        "\n",
        "update_counter(train_df)\n",
        "update_counter(test_df)\n",
        "\n",
        "# create special \"tokens\" for padding, words outside the vocab, and serperators\n",
        "special_tokens = [\"<pad>\", \"<unk>\", \"[sep]\"]\n",
        "vocab = {tok: i for i, tok in enumerate(special_tokens)}\n",
        "\n",
        "for tok, _ in counter.most_common():\n",
        "  if tok not in vocab:\n",
        "    vocab[tok] = len(vocab)\n",
        "\n",
        "PAD_IDX = vocab[\"<pad>\"]\n",
        "UNK_IDX = vocab[\"<unk>\"]\n",
        "SEP_IDX = vocab[\"[sep]\"]\n",
        "\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "\n",
        "def encode_tokens(tokens):\n",
        "  return [vocab.get(t, UNK_IDX) for t in tokens]\n",
        "\n",
        "def build_sequence(context: str, question: str, answer: str):\n",
        "  # create sequence of lemmatised tokens and limit it to maximum sequence length\n",
        "  tokens = (\n",
        "    lemmatize(tokenize(context))\n",
        "    + [\"[sep]\"]\n",
        "    + lemmatize(tokenize(question))\n",
        "    + [\"[sep]\"]\n",
        "    + lemmatize(tokenize(answer))\n",
        "  )\n",
        "  ids = encode_tokens(tokens)\n",
        "  return ids[:MAX_SEQ_LEN]"
      ],
      "metadata": {
        "id": "CU9DpoMggH74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da22dced-c198-4af2-c8af-45d87ebdd8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 18149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloaders"
      ],
      "metadata": {
        "id": "GBs8DEA1ZSQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MCQTrainDataset(Dataset):\n",
        "  # class for the training data\n",
        "  def __init__(self, df: pd.DataFrame):\n",
        "    self.df = df.reset_index(drop=True)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # gets a row by id and returns the question id, associated sequences, and the correct answer\n",
        "    row = self.df.iloc[idx]\n",
        "    context = row[\"context\"]\n",
        "    question = row[\"question\"]\n",
        "    answers = ast.literal_eval(row[\"answers\"])\n",
        "    label = int(row[\"label\"])  # 0..3\n",
        "\n",
        "    seqs = []\n",
        "    for opt in answers:\n",
        "      seq_ids = build_sequence(context, question, str(opt))\n",
        "      seqs.append(seq_ids)\n",
        "\n",
        "    return {\"id\": row[\"id\"], \"seqs\": seqs, \"label\": label}\n",
        "\n",
        "class MCQTestDataset(Dataset):\n",
        "  # class for the test data\n",
        "  def __init__(self, df: pd.DataFrame):\n",
        "    self.df = df.reset_index(drop=True)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # gets a row by id and returns the question id and associated sequences\n",
        "    row = self.df.iloc[idx]\n",
        "    context = row[\"context\"]\n",
        "    question = row[\"question\"]\n",
        "    answers = ast.literal_eval(row[\"answers\"])\n",
        "\n",
        "    seqs = []\n",
        "    for opt in answers:\n",
        "      seq_ids = build_sequence(context, question, str(opt))\n",
        "      seqs.append(seq_ids)\n",
        "\n",
        "    return {\"id\": row[\"id\"], \"seqs\": seqs}\n",
        "\n",
        "def collate_fn_train(batch):\n",
        "  # creates a batch of padded samples\n",
        "  B = len(batch)\n",
        "  O = len(batch[0][\"seqs\"])\n",
        "  max_len = max(len(seq) for item in batch for seq in item[\"seqs\"])\n",
        "\n",
        "  input_ids = torch.full(\n",
        "    (B, O, max_len),\n",
        "    PAD_IDX,\n",
        "    dtype=torch.long,\n",
        "  )\n",
        "  labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
        "  ids = [item[\"id\"] for item in batch]\n",
        "\n",
        "  for i, item in enumerate(batch):\n",
        "    for k, seq in enumerate(item[\"seqs\"]):\n",
        "      input_ids[i, k, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "  return input_ids, labels, ids\n",
        "\n",
        "def collate_fn_test(batch):\n",
        "  # create a batch of padded samples, but without label\n",
        "  B = len(batch)\n",
        "  O = len(batch[0][\"seqs\"])\n",
        "  max_len = max(len(seq) for item in batch for seq in item[\"seqs\"])\n",
        "\n",
        "  input_ids = torch.full(\n",
        "    (B, O, max_len),\n",
        "    PAD_IDX,\n",
        "    dtype=torch.long,\n",
        "  )\n",
        "  ids = [item[\"id\"] for item in batch]\n",
        "\n",
        "  for i, item in enumerate(batch):\n",
        "    for k, seq in enumerate(item[\"seqs\"]):\n",
        "      input_ids[i, k, : len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "  return input_ids, ids\n",
        "\n",
        "# create proper datasets\n",
        "train_dataset = MCQTrainDataset(train_df)\n",
        "val_dataset   = MCQTrainDataset(val_df)\n",
        "test_dataset  = MCQTestDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  shuffle=True,\n",
        "  collate_fn=collate_fn_train,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "  val_dataset,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  shuffle=False,\n",
        "  collate_fn=collate_fn_train,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  shuffle=False,\n",
        "  collate_fn=collate_fn_test,\n",
        ")"
      ],
      "metadata": {
        "id": "Zr_PLNCCgRhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BiLSTM model"
      ],
      "metadata": {
        "id": "6GzjbpA6cQR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMAttentionMCQ(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hidden_size, pad_idx,\n",
        "              dropout=0.3):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "\n",
        "    # 2-layer BiLSTM with dropout between layers\n",
        "    self.lstm = nn.LSTM(\n",
        "      emb_dim,\n",
        "      hidden_size,\n",
        "      batch_first=True,\n",
        "      bidirectional=True,\n",
        "      num_layers=3,\n",
        "      dropout=dropout,\n",
        "    )\n",
        "\n",
        "    # attention mechanism\n",
        "    self.att_w = nn.Linear(2 * hidden_size, 2 * hidden_size)\n",
        "    self.att_v = nn.Linear(2 * hidden_size, 1, bias=False)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(2 * hidden_size, 128),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout),\n",
        "      nn.Linear(128, 1),\n",
        "    )\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    \"\"\"\n",
        "    input_ids: [B, O, L]\n",
        "    returns: logits [B, O]\n",
        "    \"\"\"\n",
        "    B, O, L = input_ids.shape\n",
        "\n",
        "    # flatten options and ignore padding\n",
        "    x = input_ids.view(B * O, L)          # [B*O, L]\n",
        "    mask = x.ne(self.pad_idx)             # [B*O, L]\n",
        "\n",
        "    emb = self.embedding(x)               # [B*O, L, E]\n",
        "    h, _ = self.lstm(emb)                 # [B*O, L, 2H]\n",
        "\n",
        "    scores = self.att_v(\n",
        "      torch.tanh(self.att_w(h))\n",
        "    ).squeeze(-1)                         # [B*O, L]\n",
        "    scores = scores.masked_fill(~mask, -1e9)\n",
        "    attn = torch.softmax(scores, dim=-1).unsqueeze(-1)  # [B*O, L, 1]\n",
        "\n",
        "    context = (h * attn).sum(dim=1)       # [B*O, 2H]\n",
        "    context = self.dropout(context)\n",
        "\n",
        "    logits = self.classifier(context).view(B, O)  # [B, O]\n",
        "    return logits\n",
        "\n",
        "# init model\n",
        "model = BiLSTMAttentionMCQ(\n",
        "  vocab_size=len(vocab),\n",
        "  emb_dim=EMB_DIM,\n",
        "  hidden_size=HIDDEN_SIZE,\n",
        "  pad_idx=PAD_IDX,\n",
        "  dropout=DROPOUT\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "a-ai9yvcgWbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "eECDCCmwiPR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, epoch_idx):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "  total_examples = 0\n",
        "\n",
        "  for input_ids, labels, ids in loader:\n",
        "    input_ids = input_ids.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(input_ids)              # [B, 4]\n",
        "    loss = F.cross_entropy(logits, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item() * input_ids.size(0)\n",
        "    total_examples += input_ids.size(0)\n",
        "\n",
        "  avg_loss = total_loss / total_examples\n",
        "  print(f\"Epoch {epoch_idx}: train loss={avg_loss:.4f}\")\n",
        "\n",
        "def evaluate(model, loader):\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for input_ids, labels, ids in loader:\n",
        "      input_ids = input_ids.to(device)\n",
        "      labels = labels.to(device)\n",
        "      logits = model(input_ids)\n",
        "      preds = logits.argmax(dim=-1)\n",
        "      correct += (preds == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "  return correct / total\n",
        "\n",
        "best_val = 0.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "  train_one_epoch(model, train_loader, optimizer, epoch)\n",
        "  val_acc = evaluate(model, val_loader)\n",
        "  print(f\"Epoch {epoch}: val acc={val_acc:.4f}\")\n",
        "\n",
        "  if val_acc > best_val:\n",
        "    best_val = val_acc\n",
        "    best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "    print(f\"  -> new best model (val acc={best_val:.4f})\")\n",
        "\n",
        "# load best weights before test inference\n",
        "if best_state is not None:\n",
        "  model.load_state_dict(best_state)\n",
        "  model.to(device)\n",
        "  print(f\"Loaded best model with val acc={best_val:.4f}\")"
      ],
      "metadata": {
        "id": "k6ZiIDe7geql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577bfbe1-3ec5-447f-ee47-9095dfc99a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train loss=1.3862\n",
            "Epoch 1: val acc=0.2845\n",
            "  -> new best model (val acc=0.2845)\n",
            "Epoch 2: train loss=1.3754\n",
            "Epoch 2: val acc=0.2608\n",
            "Epoch 3: train loss=1.3124\n",
            "Epoch 3: val acc=0.3103\n",
            "  -> new best model (val acc=0.3103)\n",
            "Epoch 4: train loss=1.1811\n",
            "Epoch 4: val acc=0.2953\n",
            "Epoch 5: train loss=0.9423\n",
            "Epoch 5: val acc=0.3125\n",
            "  -> new best model (val acc=0.3125)\n",
            "Epoch 6: train loss=0.6293\n",
            "Epoch 6: val acc=0.3103\n",
            "Epoch 7: train loss=0.3545\n",
            "Epoch 7: val acc=0.3147\n",
            "  -> new best model (val acc=0.3147)\n",
            "Epoch 8: train loss=0.2088\n",
            "Epoch 8: val acc=0.3168\n",
            "  -> new best model (val acc=0.3168)\n",
            "Epoch 9: train loss=0.1196\n",
            "Epoch 9: val acc=0.3082\n",
            "Epoch 10: train loss=0.0662\n",
            "Epoch 10: val acc=0.2974\n",
            "Epoch 11: train loss=0.0978\n",
            "Epoch 11: val acc=0.3254\n",
            "  -> new best model (val acc=0.3254)\n",
            "Epoch 12: train loss=0.0688\n",
            "Epoch 12: val acc=0.3082\n",
            "Epoch 13: train loss=0.0386\n",
            "Epoch 13: val acc=0.3103\n",
            "Epoch 14: train loss=0.0428\n",
            "Epoch 14: val acc=0.3448\n",
            "  -> new best model (val acc=0.3448)\n",
            "Epoch 15: train loss=0.0217\n",
            "Epoch 15: val acc=0.3254\n",
            "Loaded best model with val acc=0.3448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Test Answers"
      ],
      "metadata": {
        "id": "okv4JrrCk147"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_ids = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for input_ids, ids in test_loader:\n",
        "    input_ids = input_ids.to(device)\n",
        "    logits = model(input_ids)                # [B, 4]\n",
        "    preds = logits.argmax(dim=-1).cpu().tolist()\n",
        "    all_ids.extend(ids)\n",
        "    all_preds.extend(preds)\n",
        "\n",
        "submission_df = pd.DataFrame({\"id\": all_ids, \"label\": all_preds})\n",
        "submission_df.to_csv(OUTPUT_SUB_PATH, index=False)\n",
        "print(\"Saved submission to\", OUTPUT_SUB_PATH)\n"
      ],
      "metadata": {
        "id": "AU-yLJU_ghA7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40dcb167-3371-4dc9-d133-63c2bb793652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission to /content/drive/MyDrive/submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iZ3peeL8-gXT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
